{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Generator for the h-space similarity explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, AutoencoderKL\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from contextlib import ExitStack\n",
    "from functools import partial\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models: list[dict[str,Any]] = [\n",
    "    dict(\n",
    "        short='SDXL-Turbo',\n",
    "        name='stabilityai/sdxl-turbo',\n",
    "        steps=4,\n",
    "        guidance_scale=0.0,\n",
    "        vae='stabilityai/sdxl-vae',\n",
    "        extract_positions = ['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'conv_out'],\n",
    "    ),\n",
    "    dict(\n",
    "        short='SD-Turbo',\n",
    "        name='stabilityai/sd-turbo',\n",
    "        steps=2,\n",
    "        guidance_scale=0.0,\n",
    "        vae='default',\n",
    "        extract_positions = ['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'conv_out'],\n",
    "    ),\n",
    "    dict(\n",
    "        short='SD-1.5',\n",
    "        name='runwayml/stable-diffusion-v1-5',\n",
    "        steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        vae='default',\n",
    "        extract_positions = ['down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]'],\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompts = {\n",
    "    \"Cat\": \"A photo of a cat.\",\n",
    "    \"Dog\": \"A photograph of a husky, dog, looking friendly and cute.\",\n",
    "    \"Polarbear\": \"A photo of a polar bear.\",\n",
    "    \"ConstructionWorker\": \"A photo of a hard working construction worker.\",\n",
    "    \"Woman\": \"A photo of a beautiful, slightly smiling woman in the city.\",\n",
    "    \"OldMan\": \"A portrait of an old man with a long beard and a hat.\",\n",
    "    \"FuturisticCityscape\": \"A futuristic cityscape at sunset, with flying cars and towering skyscrapers, in the style of cyberpunk.\",\n",
    "    \"MountainLandscape\": \"A serene mountain landscape with a crystal-clear lake in the foreground, reflecting the snow-capped peaks under a bright blue sky.\",\n",
    "    \"SpaceAstronaut\": \"A high-res photo of an astronaut floating in the vastness of space, with a colorful nebula and distant galaxies in the background.\",\n",
    "    \"MajesticLion\": \"A close-up portrait of a majestic lion, with detailed fur and piercing eyes, set against the backdrop of the African savannah at dusk.\",\n",
    "    \"MagicalForest\": \"A magical forest filled with glowing plants, mythical creatures, and a pathway leading to an enchanted castle.\",\n",
    "    \"JapaneseGarden\": \"A traditional Japanese garden in spring, complete with cherry blossoms, a koi pond, and a wooden bridge.\",\n",
    "}\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e89410393c40b9bdd082aeeb7d8fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asdf/mambaforge/envs/py311/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:959: FutureWarning: `callback` is deprecated and will be removed in version 1.0.0. Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\n",
      "  deprecate(\n",
      "/home/asdf/mambaforge/envs/py311/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:965: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\n",
      "  deprecate(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db95a8192591493b87618ced6ad74230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running SDXL-Turbo:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "get_reprs() missing 1 required positional argument: 'extract_positions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m save_path\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m representations, images \u001b[38;5;241m=\u001b[39m \u001b[43mget_reprs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguidance_scale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# save representations\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos, reprs \u001b[38;5;129;01min\u001b[39;00m representations\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mTypeError\u001b[0m: get_reprs() missing 1 required positional argument: 'extract_positions'"
     ]
    }
   ],
   "source": [
    "def get_reprs(pipe, prompt, steps, guidance_scale, extract_positions, vae=None):\n",
    "    '''Get representations and intermediate images from a model.'''\n",
    "    if vae is None: vae = pipe.vae\n",
    "    representations = {pos: [] for pos in extract_positions}\n",
    "    images = []\n",
    "    def latents_callback(i, t, latents):\n",
    "        latents = 1 / vae.config.scaling_factor * latents.to(dtype=vae.dtype)\n",
    "        image = vae.decode(latents).sample[0]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(1, 2, 0).numpy()\n",
    "        images.extend(pipe.numpy_to_pil(image))\n",
    "    \n",
    "    with ExitStack() as stack, torch.no_grad():\n",
    "        for extract_position in extract_positions:\n",
    "            def get_repr(module, input, output, extract_position):\n",
    "                representations[extract_position].append(output[0].cpu().numpy())\n",
    "            # eval is unsafe. Do not use in production.\n",
    "            stack.enter_context(eval(f'unet.{extract_position}', {'__builtins__': {}, 'unet': pipe.unet}).register_forward_hook(partial(get_repr, extract_position=extract_position)))\n",
    "        result = pipe(prompt, num_inference_steps = steps, guidance_scale=guidance_scale, callback=latents_callback, callback_steps=1, generator=torch.Generator(\"cuda\").manual_seed(seed))\n",
    "    return representations, images\n",
    "\n",
    "# setup output directory\n",
    "base_path = Path('representations')\n",
    "base_path.mkdir(exist_ok=True)\n",
    "with open(base_path / '.gitignore', 'w') as f:\n",
    "    f.write('*')\n",
    "\n",
    "# run the models\n",
    "for model_dict in models:\n",
    "    model_name = model_dict['name']\n",
    "    model_name_short = model_dict['short']\n",
    "    vae_name = model_dict['vae']\n",
    "\n",
    "    # load model\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "    pipe.set_progress_bar_config(disable=True)  # disable progress bar\n",
    "    vae = AutoencoderKL.from_pretrained(vae_name).to('cuda') if vae_name != 'default' else None\n",
    "\n",
    "    # note model h-space dimensions\n",
    "    representations, _ = get_reprs(pipe, '', 1, 0, model_dict['extract_positions'], vae=vae)\n",
    "    model_dict['representations'] = {}\n",
    "    for pos, reprs in representations.items():\n",
    "        model_dict['representations'][pos] = {\n",
    "            'channels': reprs[0].shape[-3],\n",
    "            'spatial': reprs[0].shape[-1],\n",
    "        }\n",
    "\n",
    "    # go through prompts\n",
    "    for i, (prompt_name, prompt) in enumerate(tqdm(prompts.items(), desc=f'Running {model_name_short}')):\n",
    "\n",
    "        # setup save path\n",
    "        save_path = base_path / model_name_short / prompt_name\n",
    "        save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # run the model\n",
    "        representations, images = get_reprs(pipe, prompt, model_dict['steps'], model_dict['guidance_scale'], model_dict['extract_positions'], vae=vae)\n",
    "\n",
    "        # save representations\n",
    "        for pos, reprs in representations.items():\n",
    "            with open(save_path / f'repr-{pos}.bin', 'wb') as f:\n",
    "                f.write(np.array(np.stack(reprs), dtype=np.float32).tobytes())\n",
    "\n",
    "        # save intermediate images\n",
    "        for j, img in enumerate(images, 1):\n",
    "            img.save(save_path / f'{j}.jpg')\n",
    "\n",
    "        # save config\n",
    "        git_hash = !git rev-parse main\n",
    "        with open(save_path / 'config.json', 'w') as f:\n",
    "            f.write(json.dumps({**model_dict, 'prompt_name': prompt_name, 'prompt': prompt, 'git_hash': git_hash[0], 'seed': seed}))\n",
    "\n",
    "# save global config files\n",
    "with open(base_path/'prompts.json', 'w') as f:\n",
    "    f.write(json.dumps(prompts))\n",
    "with open(base_path/'models.json', 'w') as f:\n",
    "    f.write(json.dumps(models))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
