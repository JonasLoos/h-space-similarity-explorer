{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Generator for the h-space similarity explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, AutoencoderKL\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from contextlib import ExitStack\n",
    "from functools import partial\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models: list[dict[str,Any]] = [\n",
    "    dict(\n",
    "        short='SDXL-Turbo',\n",
    "        name='stabilityai/sdxl-turbo',\n",
    "        steps=4,\n",
    "        guidance_scale=0.0,\n",
    "        extract_positions = ['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'conv_out'],\n",
    "    ),\n",
    "    dict(\n",
    "        short='SD-Turbo',\n",
    "        name='stabilityai/sd-turbo',\n",
    "        steps=2,\n",
    "        guidance_scale=0.0,\n",
    "        extract_positions = ['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'conv_out'],\n",
    "    ),\n",
    "    dict(\n",
    "        short='SD-1.5',\n",
    "        name='runwayml/stable-diffusion-v1-5',\n",
    "        steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        extract_positions = ['down_blocks[1]', 'down_blocks[2]', 'mid_block', 'up_blocks[0]'],\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompts = {\n",
    "    \"Cat\": \"A photo of a cat.\",\n",
    "    \"Dog\": \"A photograph of a husky, dog, looking friendly and cute.\",\n",
    "    \"Polarbear\": \"A photo of a polar bear.\",\n",
    "    \"ConstructionWorker\": \"A photo of a hard working construction worker.\",\n",
    "    \"Woman\": \"A photo of a beautiful, slightly smiling woman in the city.\",\n",
    "    \"OldMan\": \"A portrait of an old man with a long beard and a hat.\",\n",
    "    \"FuturisticCityscape\": \"A futuristic cityscape at sunset, with flying cars and towering skyscrapers, in the style of cyberpunk.\",\n",
    "    \"MountainLandscape\": \"A serene mountain landscape with a crystal-clear lake in the foreground, reflecting the snow-capped peaks under a bright blue sky.\",\n",
    "    \"SpaceAstronaut\": \"A high-res photo of an astronaut floating in the vastness of space, with a colorful nebula and distant galaxies in the background.\",\n",
    "    \"MajesticLion\": \"A close-up portrait of a majestic lion, with detailed fur and piercing eyes, set against the backdrop of the African savannah at dusk.\",\n",
    "    \"MagicalForest\": \"A magical forest filled with glowing plants, mythical creatures, and a pathway leading to an enchanted castle.\",\n",
    "    \"JapaneseGarden\": \"A traditional Japanese garden in spring, complete with cherry blossoms, a koi pond, and a wooden bridge.\",\n",
    "}\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reprs(pipe, prompt, steps, guidance_scale, extract_positions):\n",
    "    '''Get representations and intermediate images from a model.'''\n",
    "    representations = {pos: [] for pos in extract_positions}\n",
    "    images = []\n",
    "    def latents_callback(i, t, latents):\n",
    "        latents = 1 / pipe.vae.config.scaling_factor * latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "        image = pipe.vae.decode(latents).sample[0]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(1, 2, 0).numpy()\n",
    "        images.extend(pipe.numpy_to_pil(image))\n",
    "    \n",
    "    with ExitStack() as stack, torch.no_grad():\n",
    "        for extract_position in extract_positions:\n",
    "            def get_repr(module, input, output, extract_position):\n",
    "                representations[extract_position].append(output[0].cpu().numpy())\n",
    "            # eval is unsafe. Do not use in production.\n",
    "            stack.enter_context(eval(f'unet.{extract_position}', {'__builtins__': {}, 'unet': pipe.unet}).register_forward_hook(partial(get_repr, extract_position=extract_position)))\n",
    "        result = pipe(prompt, num_inference_steps = steps, guidance_scale=guidance_scale, callback=latents_callback, callback_steps=1, generator=torch.Generator(\"cuda\").manual_seed(seed))\n",
    "    return representations, images\n",
    "\n",
    "# setup output directory\n",
    "base_path = Path('representations')\n",
    "base_path.mkdir(exist_ok=True)\n",
    "with open(base_path / '.gitignore', 'w') as f:\n",
    "    f.write('*')\n",
    "\n",
    "# run the models\n",
    "for model_dict in models:\n",
    "    model_name = model_dict['name']\n",
    "    model_name_short = model_dict['short']\n",
    "\n",
    "    # load model\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "    pipe.set_progress_bar_config(disable=True)  # disable progress bar\n",
    "    if hasattr(pipe, 'upcast_vae') and pipe.vae.dtype == torch.float16 and pipe.vae.config.force_upcast:\n",
    "        pipe.upcast_vae()  # upcast VAE to float32 if needed (required for SDXL models)\n",
    "\n",
    "    # note model h-space dimensions\n",
    "    representations, _ = get_reprs(pipe, '', 1, 0, model_dict['extract_positions'])\n",
    "    model_dict['representations'] = {}\n",
    "    for pos, reprs in representations.items():\n",
    "        model_dict['representations'][pos] = {\n",
    "            'channels': reprs[0].shape[-3],\n",
    "            'spatial': reprs[0].shape[-1],\n",
    "        }\n",
    "\n",
    "    # go through prompts\n",
    "    for i, (prompt_name, prompt) in enumerate(tqdm(prompts.items(), desc=f'Running {model_name_short}')):\n",
    "\n",
    "        # setup save path\n",
    "        save_path = base_path / model_name_short / prompt_name\n",
    "        save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # run the model\n",
    "        representations, images = get_reprs(pipe, prompt, model_dict['steps'], model_dict['guidance_scale'], model_dict['extract_positions'])\n",
    "\n",
    "        # save representations\n",
    "        for pos, reprs in representations.items():\n",
    "            with open(save_path / f'repr-{pos}.bin', 'wb') as f:\n",
    "                f.write(np.array(np.stack(reprs), dtype=np.float32).tobytes())\n",
    "\n",
    "        # save intermediate images\n",
    "        for j, img in enumerate(images, 1):\n",
    "            img.save(save_path / f'{j}.jpg')\n",
    "\n",
    "        # save config\n",
    "        git_hash = !git rev-parse main\n",
    "        with open(save_path / 'config.json', 'w') as f:\n",
    "            f.write(json.dumps({**model_dict, 'prompt_name': prompt_name, 'prompt': prompt, 'git_hash': git_hash[0], 'seed': seed}))\n",
    "\n",
    "# save global config files\n",
    "with open(base_path/'prompts.json', 'w') as f:\n",
    "    f.write(json.dumps(prompts))\n",
    "with open(base_path/'models.json', 'w') as f:\n",
    "    f.write(json.dumps(models))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
