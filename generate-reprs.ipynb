{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Generator for the h-space similarity explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, AutoencoderKL\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    dict(\n",
    "        short='SD-1.5',\n",
    "        name='runwayml/stable-diffusion-v1-5',\n",
    "        steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        vae='default',\n",
    "    ),\n",
    "    dict(\n",
    "        short='SD-Turbo',\n",
    "        name='stabilityai/sd-turbo',\n",
    "        steps=2,\n",
    "        guidance_scale=0.0,\n",
    "        vae='default',\n",
    "    ),\n",
    "    dict(\n",
    "        short='SDXL-Turbo',\n",
    "        name='stabilityai/sdxl-turbo',\n",
    "        steps=4,\n",
    "        guidance_scale=0.0,\n",
    "        vae='stabilityai/sdxl-vae',\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompts = {\n",
    "    \"Cat\": \"A photo of a cat.\",\n",
    "    \"Dog\": \"A photograph of a husky, dog, looking friendly and cute.\",\n",
    "    \"Polarbear\": \"A photo of a polar bear.\",\n",
    "    \"ConstructionWorker\": \"A photo of a hard working construction worker.\",\n",
    "    \"Woman\": \"A photo of a beautiful, slightly smiling woman in the city.\",\n",
    "    \"OldMan\": \"A portrait of an old man with a long beard and a hat.\",\n",
    "    \"FuturisticCityscape\": \"A futuristic cityscape at sunset, with flying cars and towering skyscrapers, in the style of cyberpunk.\",\n",
    "    \"MountainLandscape\": \"A serene mountain landscape with a crystal-clear lake in the foreground, reflecting the snow-capped peaks under a bright blue sky.\",\n",
    "    \"SpaceAstronaut\": \"A high-res photo of an astronaut floating in the vastness of space, with a colorful nebula and distant galaxies in the background.\",\n",
    "    \"MajesticLion\": \"A close-up portrait of a majestic lion, with detailed fur and piercing eyes, set against the backdrop of the African savannah at dusk.\",\n",
    "    \"MagicalForest\": \"A magical forest filled with glowing plants, mythical creatures, and a pathway leading to an enchanted castle.\",\n",
    "    \"JapaneseGarden\": \"A traditional Japanese garden in spring, complete with cherry blossoms, a koi pond, and a wooden bridge.\",\n",
    "}\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reprs(pipe, prompt, steps, guidance_scale, vae=None):\n",
    "    '''Get representations and intermediate images from a model.'''\n",
    "    if vae is None: vae = pipe.vae\n",
    "    reprs = []\n",
    "    imgs = []\n",
    "    def get_repr(module, input, output):\n",
    "        reprs.append(output[0].cpu().numpy())\n",
    "    def latents_callback(i, t, latents):\n",
    "        latents = 1 / vae.config.scaling_factor * latents.to(dtype=vae.dtype)\n",
    "        image = vae.decode(latents).sample[0]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(1, 2, 0).numpy()\n",
    "        imgs.extend(pipe.numpy_to_pil(image))\n",
    "\n",
    "    with pipe.unet.mid_block.register_forward_hook(get_repr):\n",
    "        result = pipe(prompt, num_inference_steps = steps, guidance_scale=guidance_scale, callback=latents_callback, callback_steps=1, generator=torch.Generator(\"cuda\").manual_seed(seed))\n",
    "    return reprs, imgs\n",
    "\n",
    "# setup output directory\n",
    "base_path = Path('representations')\n",
    "base_path.mkdir(exist_ok=True)\n",
    "with open(base_path / '.gitignore', 'w') as f:\n",
    "    f.write('*')\n",
    "\n",
    "# run the models\n",
    "for model_dict in models:\n",
    "    model_name = model_dict['name']\n",
    "    model_name_short = model_dict['short']\n",
    "    vae_name = model_dict['vae']\n",
    "\n",
    "    # load model\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "    pipe.set_progress_bar_config(disable=True)  # disable progress bar\n",
    "    vae = AutoencoderKL.from_pretrained(vae_name).to('cuda') if vae_name != 'default' else None\n",
    "\n",
    "    # note model h-space dimensions\n",
    "    hspace_shape = get_reprs(pipe, '', 1, 0, vae)[0][0].shape\n",
    "    model_dict['hspace_channels'] = hspace_shape[0]\n",
    "    model_dict['hspace_spatial'] = hspace_shape[1]\n",
    "\n",
    "    # go through prompts\n",
    "    for i, (prompt_name, prompt) in enumerate(tqdm(prompts.items(), desc=f'Running {model_name_short}')):\n",
    "\n",
    "        # setup save path\n",
    "        save_path = base_path / model_name_short / prompt_name\n",
    "        save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # run the model\n",
    "        reprs, imgs = get_reprs(pipe, prompt, model_dict['steps'], model_dict['guidance_scale'], vae)\n",
    "\n",
    "        # save representations\n",
    "        with open(save_path / 'repr.bin', 'wb') as f:\n",
    "            f.write(np.array(np.stack(reprs), dtype=np.float32).tobytes())\n",
    "\n",
    "        # save result\n",
    "        for j, img in enumerate(imgs, 1):\n",
    "            img.save(save_path / f'{j}.png')\n",
    "\n",
    "        # save config\n",
    "        git_hash = !git rev-parse main\n",
    "        with open(save_path / 'config.json', 'w') as f:\n",
    "            f.write(json.dumps({**model_dict, 'prompt_name': prompt_name, 'prompt': prompt, 'git_hash': git_hash[0], 'seed': seed}))\n",
    "\n",
    "with open(base_path/'prompts.json', 'w') as f:\n",
    "    f.write(json.dumps(prompts))\n",
    "with open(base_path/'models.json', 'w') as f:\n",
    "    f.write(json.dumps(models))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
